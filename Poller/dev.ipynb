{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies for \"poller.py\"\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_restful import Api, Resource\n",
    "import pandas as pd\n",
    "import redis\n",
    "import pickle\n",
    "\n",
    "# import dependencies for \"extensions.py\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.utils._param_validation import InvalidParameterError\n",
    "import zlib\n",
    "import base64\n",
    "from pymongo import MongoClient\n",
    "from io import BytesIO\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "import time\n",
    "\n",
    "\n",
    "# import dependencies for \"recommender_system.py\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "import nltk\n",
    "import requests\n",
    "from datetime import datetime, timedelta, tzinfo\n",
    "\n",
    "# import dependencies for \"elasticsearch_handle.py\"\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "import elastic_transport\n",
    "\n",
    "# import dependencies for \"elastic_exceptions.py\"\n",
    "from elastic_transport import TlsError, ConnectionTimeout, ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if __name__ == \"__main__\":\\n    elasticsearch_url = \"https://159.203.183.251:9200\"\\n    username = \"pollett\"\\n    password = \"9r0&rJP@19GY\"\\n    fingerprint = \"CE:AA:F7:FF:04:C7:31:14:78:9C:62:D4:CE:98:F9:EF:56:DA:70:45:37:14:E3:F8:66:0A:25:ED:05:04:83:EC\"\\n\\n    elastic_handle = ElasticsearchHandel(\\n        elasticsearch_url, username, password, fingerprint\\n    )\\n\\n    polls = elastic_handle.get_index(\"polls\")\\n    elastic_handle.export_index_to_file(polls, \"./data/elas_polls.json\")\\n\\n    interactions = elastic_handle.get_index(\"userpollinteractions\")\\n    elastic_handle.export_index_to_file(interactions, \"./data/elas_interactions.json\")\\n    # print(polls) '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elasticsearch_handle\n",
    "class InteractionNotFound(Exception):\n",
    "    args = (\"User doesn't have any interactions.\", 110)\n",
    "\n",
    "\n",
    "class ElasticConnectionError(TlsError):\n",
    "    args = (\"Elastic connection didn't established.\", 120)\n",
    "\n",
    "\n",
    "class ElasticsearchHandel:\n",
    "    def __init__(self, elasticsearch_url, username, password, fingerprint):\n",
    "        self.elasticsearch_url = elasticsearch_url\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.fingerprint = fingerprint\n",
    "        self.client = Elasticsearch(\n",
    "            hosts=self.elasticsearch_url,\n",
    "            basic_auth=(self.username, self.password),\n",
    "            ssl_assert_fingerprint=self.fingerprint,\n",
    "        )\n",
    "\n",
    "    def get_index_v1(self, index_name, batch_size=100):\n",
    "        setattr(self, index_name, [])\n",
    "        index_list = getattr(self, index_name)\n",
    "        from_index = 0\n",
    "        all_instances = []\n",
    "\n",
    "        while True:\n",
    "            query = {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"size\": batch_size,\n",
    "                \"from\": from_index,\n",
    "            }\n",
    "            results = self.client.search(\n",
    "                index=index_name,\n",
    "                query={\"match_all\": {}},\n",
    "                size=batch_size,\n",
    "                from_=from_index,\n",
    "            )\n",
    "            instances = results[\"hits\"][\"hits\"]\n",
    "\n",
    "            all_instances.extend(instances)\n",
    "            from_index += batch_size\n",
    "            if len(instances) < 100:\n",
    "                break\n",
    "\n",
    "        setattr(self, index_name, [instance[\"_source\"] for instance in all_instances])\n",
    "        return getattr(self, index_name)\n",
    "\n",
    "    def get_index(self, index_name, batch_size=100):\n",
    "        setattr(self, index_name, [])\n",
    "\n",
    "        # Create an Elasticsearch client\n",
    "        # es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "\n",
    "        # Define a query that matches all documents\n",
    "        query = {\"query\": {\"match_all\": {}}}\n",
    "\n",
    "        # Use the count API to get the total count of documents\n",
    "        result = self.client.count(index=index_name, body=query)\n",
    "\n",
    "        # Access the total count\n",
    "        total_count = result[\"count\"]\n",
    "        print(f\"Total count of documents in index {index_name}: {total_count}\")\n",
    "\n",
    "        from_index = 0\n",
    "        all_instances = []\n",
    "\n",
    "        query = {\n",
    "            \"query\": {\"match_all\": {}},\n",
    "            \"size\": batch_size,\n",
    "            \"from\": from_index,\n",
    "        }\n",
    "        results = self.client.search(\n",
    "            index=index_name,\n",
    "            query={\"match_all\": {}},\n",
    "            size=total_count,\n",
    "            from_=from_index,\n",
    "        )\n",
    "        instances = results[\"hits\"][\"hits\"]\n",
    "\n",
    "        all_instances.extend(instances)\n",
    "        from_index += batch_size\n",
    "\n",
    "        setattr(self, index_name, [instance[\"_source\"] for instance in all_instances])\n",
    "        return getattr(self, index_name)\n",
    "\n",
    "    def get_interactions(self, index_name, user_id, batch_size=100):\n",
    "        # setattr(self, index_name, [])\n",
    "        # index_list = getattr(self, index_name)\n",
    "        from_index = 0\n",
    "        all_instances = []\n",
    "\n",
    "        query = {\n",
    "            \"match_phrase\": {\"userId\": user_id},\n",
    "        }\n",
    "\n",
    "        results = self.client.search(\n",
    "            index=index_name,\n",
    "            query=query,\n",
    "            size=batch_size,\n",
    "            from_=from_index,\n",
    "            timeout=\"1s\",\n",
    "        )\n",
    "        # instances = results[\"hits\"][\"hits\"][0]\n",
    "        hits = results[\"hits\"].get(\"hits\")\n",
    "\n",
    "        if not hits:\n",
    "            # raise ValueError(\"User doesn't have any interactions.\")\n",
    "            raise InteractionNotFound()\n",
    "\n",
    "        return hits[0].get(\"_source\")\n",
    "\n",
    "    def get_user_network_polls(self, user_id, batch_size=100):\n",
    "        # setattr(self, index_name, [])\n",
    "        # index_list = getattr(self, index_name)\n",
    "        from_index = 0\n",
    "        all_instances = []\n",
    "\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"filter\": {\"term\": {\"userPrivatePolls.keyword\": user_id}},\n",
    "                    \"must\": {\n",
    "                        \"nested\": {\n",
    "                            \"path\": \"userFolllowingIds\",\n",
    "                            \"query\": {\n",
    "                                \"terms\": {\n",
    "                                    \"userFolllowingIds.keyword\": [\n",
    "                                        \"list\",\n",
    "                                        \"of\",\n",
    "                                        \"following\",\n",
    "                                        \"user\",\n",
    "                                        \"ids\",\n",
    "                                    ]\n",
    "                                }\n",
    "                            },\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        results = self.client.search(\n",
    "            index=\"users\",\n",
    "            query=query,\n",
    "            size=batch_size,\n",
    "            from_=from_index,\n",
    "            timeout=\"1s\",\n",
    "        )\n",
    "        # instances = results[\"hits\"][\"hits\"][0]\n",
    "        hits = results[\"hits\"].get(\"hits\")\n",
    "\n",
    "        if not hits:\n",
    "            # raise ValueError(\"User doesn't have any interactions.\")\n",
    "            raise InteractionNotFound()\n",
    "\n",
    "        return hits[0].get(\"_source\")\n",
    "\n",
    "    def get_trend_polls(self, polls, ret_list=True):\n",
    "        # polls = getattr(self, \"polls\")\n",
    "        # trend_polls = sorted(polls, key=lambda x: (-x[\"numberOfPollups\"], -x[\"numberOfVotes\"], -x[\"numberOfLike\"]))\n",
    "        trend_polls = sorted(\n",
    "            polls,\n",
    "            key=lambda x: (\n",
    "                -x[\"numberOfVotes\"],\n",
    "                -x[\"numberOfLike\"],\n",
    "                # -x[\"numberOfPollUp\"],\n",
    "            ),\n",
    "        )\n",
    "        return trend_polls\n",
    "\n",
    "        # recs = trend_polls[\"id\"]\n",
    "\n",
    "        # print(\"\\n\", filtered_trend_polls, \"\\n\")\n",
    "        # setattr(self, \"trend_polls\", trend_polls)\n",
    "\n",
    "    def export_index_to_file(self, index, index_file_path):\n",
    "        try:\n",
    "            with open(index_file_path, \"w\") as output:\n",
    "                # for instance in self.instances:\n",
    "                #        json.dump(instance[\"_source\"], output, indent=4)\n",
    "                json.dump(index, output, indent=4)\n",
    "        except Exception as exp:\n",
    "            print(\"Export Error\", exp)\n",
    "\n",
    "\n",
    "def get_index(self, index_name, batch_size=100):\n",
    "    setattr(self, index_name, [])\n",
    "    index_list = getattr(self, index_name)\n",
    "    from_index = 0\n",
    "    all_instances = []\n",
    "\n",
    "    while True:\n",
    "        # query = {\"query\": {\"match_all\": {}}, \"size\": batch_size, \"from\": from_index}\n",
    "        results = self.client.search(\n",
    "            index=index_name,\n",
    "            query={\"match_all\": {}},\n",
    "            size=batch_size,\n",
    "            from_=from_index,\n",
    "        )\n",
    "        instances = results[\"hits\"][\"hits\"]\n",
    "\n",
    "        all_instances.extend(instances)\n",
    "        from_index += batch_size\n",
    "        if len(instances) < 100:\n",
    "            break\n",
    "\n",
    "    setattr(self, index_name, [instance[\"_source\"] for instance in all_instances])\n",
    "    return getattr(self, index_name)\n",
    "\n",
    "\n",
    "\"\"\" if __name__ == \"__main__\":\n",
    "    elasticsearch_url = \"https://159.203.183.251:9200\"\n",
    "    username = \"pollett\"\n",
    "    password = \"9r0&rJP@19GY\"\n",
    "    fingerprint = \"CE:AA:F7:FF:04:C7:31:14:78:9C:62:D4:CE:98:F9:EF:56:DA:70:45:37:14:E3:F8:66:0A:25:ED:05:04:83:EC\"\n",
    "\n",
    "    elastic_handle = ElasticsearchHandel(\n",
    "        elasticsearch_url, username, password, fingerprint\n",
    "    )\n",
    "\n",
    "    polls = elastic_handle.get_index(\"polls\")\n",
    "    elastic_handle.export_index_to_file(polls, \"./data/elas_polls.json\")\n",
    "\n",
    "    interactions = elastic_handle.get_index(\"userpollinteractions\")\n",
    "    elastic_handle.export_index_to_file(interactions, \"./data/elas_interactions.json\")\n",
    "    # print(polls) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extensions\n",
    "def create_redis_pool(host, port, db):\n",
    "    return redis.ConnectionPool(host=host, port=port, db=db)\n",
    "\n",
    "\n",
    "def create_elastic_connection(\n",
    "    poller_elasticsearch_url, poller_username, poller_password, poller_fingerprint\n",
    "):\n",
    "    try:\n",
    "        elasticsearch_url = os.environ.get(poller_elasticsearch_url)\n",
    "        username = os.environ.get(poller_username)\n",
    "        password = os.environ.get(poller_password)\n",
    "        fingerprint = os.environ.get(poller_fingerprint)\n",
    "        elastic_handle = ElasticsearchHandel(\n",
    "            elasticsearch_url, username, password, fingerprint\n",
    "        )\n",
    "        if elasticsearch_url and username and password and fingerprint:\n",
    "            print(\n",
    "                \"[2. Environment variables were read correctly through (enivronment variables).]\"\n",
    "            )\n",
    "            return elastic_handle\n",
    "    except ConnectionTimeout as e:\n",
    "        load_dotenv()\n",
    "        elasticsearch_url = os.getenv(\"POLLER_ELASTICSEARCH_URL\")\n",
    "        username = os.getenv(\"POLLER_USERNAME\")\n",
    "        password = os.getenv(\"POLLER_PASSWORD\")\n",
    "        fingerprint = os.getenv(\"POLLER_FINGERPRINT\")\n",
    "\n",
    "        try:\n",
    "            elastic_handle = ElasticsearchHandel(\n",
    "                elasticsearch_url, username, password, fingerprint\n",
    "            )\n",
    "            if elasticsearch_url and username and password and fingerprint:\n",
    "                print(\n",
    "                    \"[2. Environment variables were read correctly through (getenv).]\"\n",
    "                )\n",
    "            return elastic_handle\n",
    "        except TypeError:\n",
    "            print(\"[2. Failed to read environment variables.]\")\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def remove_duplicates(input_list):\n",
    "    seen = set()\n",
    "    result = []\n",
    "\n",
    "    for item in input_list:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_entity(redis_client, entity_key, extend_expiration=600):\n",
    "    # Get the entity from Redis\n",
    "    entity = redis_client.get(entity_key)\n",
    "\n",
    "    # If the entity exists, reset the expiration time (e.g., to 60 seconds)\n",
    "\n",
    "    if entity:\n",
    "        redis_client.expire(entity_key, extend_expiration)\n",
    "        print(f\"The data for {entity_key} exists in Redis.\")\n",
    "        return entity\n",
    "\n",
    "\n",
    "def check_key_exists(redis_client, key):\n",
    "    if not redis_client.exists(key):\n",
    "        raise KeyError(f\"The key '{key}' does not exist in Redis.\")\n",
    "\n",
    "\n",
    "def find_duplicates(lst):\n",
    "    seen = set()\n",
    "    duplicates = set()\n",
    "    for item in lst:\n",
    "        if item in seen:\n",
    "            duplicates.add(item)\n",
    "        else:\n",
    "            seen.add(item)\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "def save_matrix_to_mongodb(\n",
    "    polls_tf_idf_matrix,\n",
    "    collection,\n",
    "    user_id,\n",
    "    polls_df,\n",
    "    filtered_trend_polls_list,\n",
    "    timer=False,\n",
    "):\n",
    "    # Save the sparse matrix to a BytesIO buffer\n",
    "    buffer = BytesIO()\n",
    "\n",
    "    start_time = time.time()\n",
    "    save_npz(buffer, polls_tf_idf_matrix)\n",
    "    elapse_npz_time = time.time() - start_time\n",
    "\n",
    "    # Reset the buffer position to the beginning\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Read the buffer content into binary data\n",
    "    start_time = time.time()\n",
    "    binary_data = buffer.read()\n",
    "    binary_data_time = time.time() - start_time\n",
    "\n",
    "    # Compress the binary data\n",
    "    start_time = time.time()\n",
    "    compressed_data = zlib.compress(binary_data)\n",
    "    compressed_data_time = time.time() - start_time\n",
    "\n",
    "    # Encode the compressed data as base64 for BSON storage\n",
    "    start_time = time.time()\n",
    "    encoded_data = base64.b64encode(compressed_data).decode(\"utf-8\")\n",
    "    encoded_data_time = time.time() - start_time\n",
    "\n",
    "    polls_dict = polls_df.to_dict(orient=\"records\")\n",
    "\n",
    "    # Insert the encoded data into MongoDB\n",
    "    start_time = time.time()\n",
    "    collection.insert_one(\n",
    "        {\n",
    "            \"user_id\": user_id,\n",
    "            \"polls_tf_idf_matrix\": encoded_data,\n",
    "            # \"concatenated_df\": polls_dict,\n",
    "            \"filtered_trend_polls_list\": filtered_trend_polls_list,\n",
    "        }\n",
    "    )\n",
    "    insert_one_time = time.time() - start_time\n",
    "    if timer:\n",
    "        print(f\"Function 'save_npz' took {elapse_npz_time:.4f} seconds.\")\n",
    "        print(f\"Function 'buffer.read' took {binary_data_time:.4f} seconds.\")\n",
    "        print(f\"Function 'zlib.compress' took {compressed_data_time:.4f} seconds.\")\n",
    "        print(f\"Function 'base64.b64encode' took {encoded_data_time:.4f} seconds.\")\n",
    "        print(f\"Function 'insert_one_time' took {insert_one_time:.4f} seconds.\")\n",
    "\n",
    "\n",
    "def read_matrix_from_mongodb(collection, user_id):\n",
    "    # Retrieve the document from MongoDB\n",
    "    result = collection.find_one({\"user_id\": user_id})\n",
    "\n",
    "    if result:\n",
    "        # Decode the base64 data\n",
    "        encoded_data = result.get(\"polls_tf_idf_matrix\", \"\")\n",
    "        compressed_data = base64.b64decode(encoded_data)\n",
    "\n",
    "        # Decompress the data\n",
    "        binary_data = zlib.decompress(compressed_data)\n",
    "\n",
    "        # Create a BytesIO object from the binary data\n",
    "        buffer = BytesIO(binary_data)\n",
    "\n",
    "        # Load the sparse matrix from the BytesIO buffer\n",
    "        polls_tf_idf_matrix = load_npz(buffer)\n",
    "\n",
    "        # Create a DataFrame from the concatenated_df\n",
    "        concatenated_df = pd.DataFrame(result.get(\"concatenated_df\", []))\n",
    "\n",
    "        # Get other values\n",
    "        user_id = result.get(\"user_id\", \"\")\n",
    "        filtered_trend_polls_list = result.get(\"filtered_trend_polls_list\", [])\n",
    "\n",
    "        return {\n",
    "            \"user_id\": user_id,\n",
    "            \"polls_tf_idf_matrix\": polls_tf_idf_matrix,\n",
    "            \"concatenated_df\": concatenated_df,\n",
    "            \"filtered_trend_polls_list\": filtered_trend_polls_list,\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_matrix_to_mongodb_file(matrix, collection, user_id):\n",
    "    # Save the sparse matrix to a compressed file\n",
    "    save_npz(\"matrix.npz\", matrix)\n",
    "\n",
    "    # Read the compressed file into a binary stream\n",
    "    with open(\"matrix.npz\", \"rb\") as file:\n",
    "        binary_data = file.read()\n",
    "\n",
    "    # Compress the binary data\n",
    "    compressed_data = zlib.compress(binary_data)\n",
    "\n",
    "    # Encode the compressed data as base64 for BSON storage\n",
    "    encoded_data = base64.b64encode(compressed_data).decode(\"utf-8\")\n",
    "\n",
    "    # Insert the encoded data into MongoDB\n",
    "    collection.insert_one({\"user_id\": user_id, \"sparse_matrix\": encoded_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eyz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/eyz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# recommender_system\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "tf_idf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "\n",
    "def encode_topics(df):\n",
    "    # topics = df[\"topics\"].str.get_dummies(sep=\",\")\n",
    "    # topics = df[\"topics\"].apply( topicfor topic in topics  )\n",
    "    one_hot_encoded = (\n",
    "        pd.get_dummies(df[\"topics\"].apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "    )\n",
    "    df = pd.concat([df, one_hot_encoded], axis=1)\n",
    "    # print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def set_index(df, index_column=\"poll_ID\"):\n",
    "    df.set_index(index_column, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reset_index(df):\n",
    "    df.reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_column_type(df, column_name, check_type):\n",
    "    column_index = df.columns.get_loc(column_name)\n",
    "    for i in range(len(df)):\n",
    "        if not isinstance(df.iloc[i, column_index], check_type):\n",
    "            print(\n",
    "                f\"error: {df.iloc[i, 0], df.iloc[i, 1],df.iloc[i, 2], df.iloc[i, 3], df.iloc[i, 4]}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    # tokens = [word.lower() for word in tokens if type(word) is str]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    processed_text = \" \".join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def preprocess_list(field_list):\n",
    "    ret_list = []\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    for item in field_list:\n",
    "        tokens = nltk.tokenize.word_tokenize(item)\n",
    "        # tokens = [word.lower() for word in tokens if type(word) is str]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in string.punctuation]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        processed_text = \" \".join(tokens)\n",
    "        ret_list.append(processed_text)\n",
    "\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "def create_tf_idf_matrix(df, column):\n",
    "    # print(f\"{df[column]} is {df[column].dtype} and {df[column].dtype is list} {list}: \")\n",
    "    df[column] = df[column].apply(lambda x: \" \".join(x))\n",
    "    df[column] = df[column].apply(preprocess_text)\n",
    "\n",
    "    return tf_idf.fit_transform(df[column])\n",
    "\n",
    "\n",
    "def create_souped_tf_idf_matrix(df):\n",
    "    df[\"topics\"] = df[\"topics\"].apply(preprocess_list)\n",
    "    df[\"question\"] = df[\"question\"].apply(preprocess_text)\n",
    "\n",
    "    # Create a new soup feature\n",
    "    df[\"soup\"] = df.apply(create_soup, axis=1)\n",
    "\n",
    "    return tf_idf.fit_transform(df[\"soup\"])\n",
    "\n",
    "\n",
    "def create_soup(df):\n",
    "    res = (\n",
    "        df[\"question\"]\n",
    "        + \" \"\n",
    "        + \" \".join(df[\"options\"])\n",
    "        + \" \"\n",
    "        + (4 * (\" \" + \" \".join(df[\"topics\"])))\n",
    "    )\n",
    "    # print(f\"-----------------------------------\\n* Processing: [{ }]\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def calc_cosine_similarity_matrix(tf_idf_matrix_1, tf_idf_matrix_2):\n",
    "    # if tf_idf_matrix_1 is not None and tf_idf_matrix_2 is not None:\n",
    "    return cosine_similarity(tf_idf_matrix_1, tf_idf_matrix_2)\n",
    "\n",
    "\n",
    "def id_to_index(df, search_id):\n",
    "    result = df[df[\"id\"] == str(search_id)].index.values[0]\n",
    "    print(result)\n",
    "\n",
    "    if len(result) > 0:\n",
    "        return result\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def id_to_index2(df, id):\n",
    "    try:\n",
    "        if any(df[\"id\"] == str(id)):\n",
    "            # df.to_csv(\"df.csv\", index=False)\n",
    "            # print(\n",
    "            #    f\"---------------\\nFound {id} at {df[df['id'] == str(id)].index.values[0]}\"\n",
    "            # )\n",
    "            # print(f\"\\nWhich is equal to:\\n{df[df['id'] == str(id)]}\")\n",
    "            return df[df[\"id\"] == str(id)].index.values[0]\n",
    "\n",
    "    except IndexError as e:\n",
    "        print(f\"erorrrrrrrrrrrrr:\")\n",
    "        print(f\"{str(id)}\")\n",
    "        print(f\"{df['id']==str(id)}\")\n",
    "\n",
    "\n",
    "def title_from_idx(df, idx):\n",
    "    return df[df.index == idx]\n",
    "\n",
    "\n",
    "def gen_recommendations(\n",
    "    index,\n",
    "    df,\n",
    "    cosine_similarity_matrix,\n",
    "    number_of_recommendations,\n",
    "):\n",
    "    # index = idx_from_title(df, original_title)\n",
    "    similarity_scores = list(enumerate(cosine_similarity_matrix[index]))\n",
    "    similarity_scores_sorted = sorted(\n",
    "        similarity_scores, key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    recommendations_indices = [\n",
    "        t[0] for t in similarity_scores_sorted[1 : (number_of_recommendations + 1)]\n",
    "    ]\n",
    "    recommendations = list(df[\"title\"].iloc[recommendations_indices])\n",
    "    # print(recommendations)\n",
    "    # print(similarity_scores_sorted, type(similarity_scores_sorted))\n",
    "    # recommendations_indices = [\n",
    "    #    t[0] for t in similarity_scores_sorted[1 : (number_of_recommendations + 1)]\n",
    "    # ]\n",
    "    # recommendations_scores = [\n",
    "    #    t[1] for t in similarity_scores_sorted[1 : (number_of_recommendations + 1)]\n",
    "    # ]\n",
    "    # return (df[\"title\"].iloc[recommendations_indices], recommendations_scores)\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "def gen_rec_from_list_of_polls(\n",
    "    interacted_polls,\n",
    "    filtered_polls_df,\n",
    "    cosine_similarity_matrix,\n",
    "    number_of_recommendations,\n",
    "):\n",
    "    recommendations = []\n",
    "    for poll_id in interacted_polls:\n",
    "        index = id_to_index2(filtered_polls_df, poll_id)\n",
    "        if index is not None:\n",
    "            similarity_scores = list(enumerate(cosine_similarity_matrix[index]))\n",
    "            similarity_scores_sorted = sorted(\n",
    "                similarity_scores, key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "\n",
    "            recommendations_indices = [\n",
    "                t[0]\n",
    "                for t in similarity_scores_sorted[1 : (number_of_recommendations + 1)]\n",
    "            ]\n",
    "            recs = list(filtered_polls_df[\"id\"].iloc[recommendations_indices])\n",
    "\n",
    "            # Filter out polls that have already been interacted with\n",
    "            filtered_recs = [poll for poll in recs if poll not in interacted_polls]\n",
    "\n",
    "            recommendations.append(filtered_recs)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # index = id_to_index(polls, poll_id)\n",
    "        # print(f\"cosine_similarity_matrix:{len(cosine_similarity_matrix)}\")\n",
    "        # print(f\"index:{index} | id:{poll_id}\")\n",
    "\n",
    "    flattened_recommendations = [\n",
    "        item for sublist in recommendations for item in sublist\n",
    "    ]\n",
    "    flattened_recommendations = Counter(flattened_recommendations)\n",
    "    n_most_recommended = flattened_recommendations.most_common(\n",
    "        number_of_recommendations\n",
    "    )\n",
    "    n_most_recommended = [t[0] for t in n_most_recommended]\n",
    "    # print(n_most_recommended)\n",
    "\n",
    "    return n_most_recommended\n",
    "\n",
    "\n",
    "def gen_rec_from_list_of_polls_df(\n",
    "    interacted_polls,\n",
    "    filtered_polls_df,\n",
    "    cosine_similarity_matrix,\n",
    "    number_of_recommendations,\n",
    "):\n",
    "    recommendations = []\n",
    "    for poll_id in interacted_polls:\n",
    "        index = id_to_index2(filtered_polls_df, poll_id)\n",
    "        if index is not None:\n",
    "            similarity_scores = list(enumerate(cosine_similarity_matrix[index]))\n",
    "            similarity_scores_sorted = sorted(\n",
    "                similarity_scores, key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "\n",
    "            recommendations_indices = [\n",
    "                t[0]\n",
    "                for t in similarity_scores_sorted[1 : (number_of_recommendations + 1)]\n",
    "            ]\n",
    "            recs = list(filtered_polls_df[\"id\"].iloc[recommendations_indices])\n",
    "\n",
    "            # Filter out polls that have already been interacted with\n",
    "            filtered_recs = [poll for poll in recs if poll not in interacted_polls]\n",
    "\n",
    "            recommendations.append(filtered_recs)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # index = id_to_index(polls, poll_id)\n",
    "        # print(f\"cosine_similarity_matrix:{len(cosine_similarity_matrix)}\")\n",
    "        # print(f\"index:{index} | id:{poll_id}\")\n",
    "\n",
    "    flattened_recommendations = [\n",
    "        item for sublist in recommendations for item in sublist\n",
    "    ]\n",
    "    flattened_recommendations = Counter(flattened_recommendations)\n",
    "    n_most_recommended = flattened_recommendations.most_common(\n",
    "        number_of_recommendations\n",
    "    )\n",
    "    n_most_recommended = [t[0] for t in n_most_recommended]\n",
    "\n",
    "    filtered_df = filtered_polls_df[filtered_polls_df[\"id\"].isin(n_most_recommended)]\n",
    "\n",
    "    order_dict = {id: idx for idx, id in enumerate(n_most_recommended)}\n",
    "\n",
    "    # Sort the filtered DataFrame based on the order\n",
    "    # filtered_df[\"order\"] = filtered_df[\"id\"].map(order_dict)\n",
    "    filtered_df.loc[:, \"order\"] = filtered_df.loc[:, \"id\"].map(order_dict)\n",
    "\n",
    "    filtered_df = filtered_df.sort_values(\"order\")\n",
    "\n",
    "    # Drop the 'order' column if not needed\n",
    "    filtered_df = filtered_df.drop(columns=[\"order\"])\n",
    "\n",
    "    # Reset the index if needed\n",
    "    filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def is_valid_limitations(limitations):\n",
    "    if isinstance(limitations, dict):\n",
    "        return (\n",
    "            \"allowedLocations\" in limitations\n",
    "            and \"allowedGender\" in limitations\n",
    "            and \"allowedAgeRange\" in limitations\n",
    "        )\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_within_x_days_liifetime(timestamp):\n",
    "    try:\n",
    "        # Convert the timestamp to a datetime object\n",
    "        time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "        # Calculate the time difference\n",
    "        time_difference = datetime.now() - time\n",
    "\n",
    "        return True if time_difference <= timedelta(days=10) else False\n",
    "    except ValueError:\n",
    "        # If the timestamp doesn't match the expected format, return False\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_timestamp(timestamp):\n",
    "    try:\n",
    "        # Convert the timestamp to a datetime object\n",
    "        time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "        # Calculate the time difference\n",
    "        time_difference = datetime.now() - time\n",
    "\n",
    "        return time_difference\n",
    "    except ValueError:\n",
    "        # If the timestamp doesn't match the expected format, return None\n",
    "        return None\n",
    "\n",
    "\n",
    "def split_by_days(polls_df, days=10):\n",
    "    filtered_df = polls_df[polls_df[\"createdAt\"].apply(filter_timestamp).notna()]\n",
    "\n",
    "    older_than_x_days = filtered_df[\n",
    "        filtered_df[\"createdAt\"].apply(filter_timestamp) >= timedelta(days=days)\n",
    "    ]\n",
    "    newer_than_x_days = filtered_df[\n",
    "        filtered_df[\"createdAt\"].apply(filter_timestamp) < timedelta(days=days)\n",
    "    ]\n",
    "\n",
    "    # Reset the index if needed\n",
    "    older_than_x_days = older_than_x_days.reset_index(drop=True)\n",
    "    newer_than_x_days = newer_than_x_days.reset_index(drop=True)\n",
    "\n",
    "    return older_than_x_days, newer_than_x_days\n",
    "\n",
    "\n",
    "def has_valid_date(date_str):\n",
    "    # Convert the date string to a datetime object\n",
    "    # date = pd.to_datetime(date_str)\n",
    "    date = pd.to_datetime(date_str, utc=True).replace(tzinfo=None)\n",
    "\n",
    "    # Get the current timestamp as a datetime object\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    # Compare the date with the current timestamp\n",
    "    return date > current_time\n",
    "\n",
    "\n",
    "def remove_duplicates(input_list):\n",
    "    seen = set()\n",
    "    result = []\n",
    "\n",
    "    for item in input_list:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def validate_polls_v1(polls_df, df_name, verbose=True):\n",
    "    valid_polls = polls_df.loc[\n",
    "        polls_df[\"valid\"] & polls_df[\"endedAt\"].apply(has_valid_date)\n",
    "    ]\n",
    "\n",
    "    invalid_polls = polls_df[\n",
    "        ~polls_df[\"valid\"] | ~polls_df[\"endedAt\"].apply(has_valid_date)\n",
    "    ]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"valid_{df_name}_polls: {len(valid_polls)}\")\n",
    "        print(f\"expired_{df_name}_polls: {len(invalid_polls)}\")\n",
    "\n",
    "    return valid_polls, invalid_polls\n",
    "    return pd.concat([valid_polls, invalid_polls], ignore_index=False)\n",
    "\n",
    "\n",
    "def validate_polls(polls_df, df_name, verbose=True):\n",
    "    valid_polls = polls_df.loc[\n",
    "        polls_df[\"valid\"] & polls_df[\"endedAt\"].apply(has_valid_date)\n",
    "    ]\n",
    "\n",
    "    invalid_polls = polls_df[~polls_df[\"id\"].isin(valid_polls[\"id\"].tolist())]\n",
    "    # invalid_polls = polls_df.loc[\n",
    "    #    ~polls_df[\"valid\"] | ~polls_df[\"endedAt\"].apply(has_valid_date)\n",
    "    # ]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"valid_{df_name}_polls: {len(valid_polls)}\")\n",
    "        print(f\"expired_{df_name}_polls: {len(invalid_polls)}\")\n",
    "\n",
    "    return valid_polls, invalid_polls\n",
    "\n",
    "\n",
    "def order_v1(\n",
    "    recommended_polls_df=None,\n",
    "    trend_polls_df=None,\n",
    "    live_polls_flag=0,\n",
    "    verbose=True,\n",
    "):\n",
    "    if not live_polls_flag:\n",
    "        if recommended_polls_df is None:\n",
    "            valid_trend_polls, invalid_trend_polls = validate_polls_v1(\n",
    "                trend_polls_df, \"trend\", verbose\n",
    "            )\n",
    "            recommended_polls_df = pd.concat(\n",
    "                [\n",
    "                    valid_trend_polls,\n",
    "                    invalid_trend_polls,\n",
    "                ],\n",
    "                ignore_index=False,\n",
    "            )\n",
    "\n",
    "        elif trend_polls_df is None:\n",
    "            valid_recommended_polls, invalid_recommended_polls = validate_polls_v1(\n",
    "                recommended_polls_df, \"recommended\", verbose\n",
    "            )\n",
    "            recommended_polls_df = pd.concat(\n",
    "                [\n",
    "                    valid_recommended_polls,\n",
    "                    invalid_recommended_polls,\n",
    "                ],\n",
    "                ignore_index=False,\n",
    "            )\n",
    "\n",
    "        elif trend_polls_df is not None and recommended_polls_df is not None:\n",
    "            valid_recommended_polls, invalid_recommended_polls = validate_polls_v1(\n",
    "                recommended_polls_df, \"recommended\", verbose\n",
    "            )\n",
    "            valid_trend_polls, invalid_trend_polls = validate_polls_v1(\n",
    "                trend_polls_df, \"trend\", verbose\n",
    "            )\n",
    "\n",
    "            recommended_polls_df = pd.concat(\n",
    "                [\n",
    "                    valid_recommended_polls,\n",
    "                    valid_trend_polls,\n",
    "                    invalid_recommended_polls,\n",
    "                    invalid_trend_polls,\n",
    "                ],\n",
    "                ignore_index=False,\n",
    "            )\n",
    "\n",
    "        recommended_polls_df = recommended_polls_df.reset_index(drop=True)\n",
    "        recommended_polls_df = recommended_polls_df[\"id\"].tolist()\n",
    "        recommended_polls_df = remove_duplicates(recommended_polls_df)\n",
    "\n",
    "        return recommended_polls_df\n",
    "\n",
    "    else:\n",
    "        if recommended_polls_df is None:\n",
    "            recommended_polls_df, _ = validate_polls_v1(\n",
    "                trend_polls_df, \"trend\", verbose\n",
    "            )\n",
    "\n",
    "        elif trend_polls_df is None:\n",
    "            recommended_polls_df, _ = validate_polls_v1(\n",
    "                recommended_polls_df, \"recommended\", verbose\n",
    "            )\n",
    "\n",
    "        elif trend_polls_df is not None and recommended_polls_df is not None:\n",
    "            valid_recommended_polls, invalid_recommended_polls = validate_polls_v1(\n",
    "                recommended_polls_df, \"recommended\", verbose\n",
    "            )\n",
    "            valid_trend_polls, invalid_trend_polls = validate_polls_v1(\n",
    "                trend_polls_df, \"trend\", verbose\n",
    "            )\n",
    "\n",
    "            recommended_polls_df = pd.concat(\n",
    "                [\n",
    "                    valid_recommended_polls,\n",
    "                    valid_trend_polls,\n",
    "                ],\n",
    "                ignore_index=False,\n",
    "            )\n",
    "\n",
    "        recommended_polls_df = recommended_polls_df.reset_index(drop=True)\n",
    "        recommended_polls_df = recommended_polls_df[\"id\"].tolist()\n",
    "        recommended_polls_df = remove_duplicates(recommended_polls_df)\n",
    "\n",
    "        return recommended_polls_df\n",
    "\n",
    "\n",
    "def order_v2(\n",
    "    recommended_polls_df=None,\n",
    "    trend_polls_df=None,\n",
    "    live_polls_flag=False,\n",
    "    verbose=True,\n",
    "):\n",
    "    if live_polls_flag:\n",
    "        if recommended_polls_df is None:\n",
    "            recommended_polls_df = validate_and_concat(trend_polls_df, \"trend\", verbose)\n",
    "\n",
    "        elif trend_polls_df is not None:\n",
    "            valid_recommended_polls = validate_and_concat(\n",
    "                recommended_polls_df, \"recommended\", verbose\n",
    "            )\n",
    "            valid_trend_polls = validate_and_concat(trend_polls_df, \"trend\", verbose)\n",
    "\n",
    "            recommended_polls_df = pd.concat(\n",
    "                [valid_recommended_polls, valid_trend_polls], ignore_index=False\n",
    "            )\n",
    "\n",
    "        recommended_polls_df = recommended_polls_df.reset_index(drop=True)\n",
    "        recommended_polls_df = recommended_polls_df[\"id\"].tolist()\n",
    "        recommended_polls_df = remove_duplicates(recommended_polls_df)\n",
    "\n",
    "        return recommended_polls_df\n",
    "\n",
    "    else:\n",
    "        if recommended_polls_df is None:\n",
    "            recommended_polls_df = validate_and_concat(trend_polls_df, \"trend\", verbose)\n",
    "\n",
    "        elif trend_polls_df is None:\n",
    "            recommended_polls_df = validate_and_concat(\n",
    "                recommended_polls_df, \"recommended\", verbose\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            valid_recommended_polls = validate_and_concat(\n",
    "                recommended_polls_df, \"recommended\", verbose\n",
    "            )\n",
    "            valid_trend_polls = validate_and_concat(trend_polls_df, \"trend\", verbose)\n",
    "\n",
    "            recommended_polls_df = pd.concat(\n",
    "                [valid_recommended_polls, valid_trend_polls], ignore_index=False\n",
    "            )\n",
    "\n",
    "        recommended_polls_df = recommended_polls_df.reset_index(drop=True)\n",
    "        recommended_polls_df = recommended_polls_df[\"id\"].tolist()\n",
    "        recommended_polls_df = remove_duplicates(recommended_polls_df)\n",
    "\n",
    "        return recommended_polls_df\n",
    "\n",
    "\n",
    "def order_v3(\n",
    "    recommended_polls_df=None,\n",
    "    trend_polls_df=None,\n",
    "    live_polls_flag=0,\n",
    "    verbose=True,\n",
    "):\n",
    "    val_recommended_polls_df, inval_recommended_polls_df = validate_polls(\n",
    "        recommended_polls_df, \"recommended\", verbose\n",
    "    )\n",
    "\n",
    "    val_trend_polls_df, inval_trend_polls_df = validate_polls(\n",
    "        trend_polls_df, \"trend\", verbose\n",
    "    )\n",
    "\n",
    "    if live_polls_flag:\n",
    "        recommended_polls_df = pd.concat(\n",
    "            [\n",
    "                val_recommended_polls_df,\n",
    "                val_trend_polls_df,\n",
    "            ],\n",
    "            ignore_index=False,\n",
    "        )\n",
    "    else:\n",
    "        recommended_polls_df = pd.concat(\n",
    "            [\n",
    "                val_recommended_polls_df,\n",
    "                val_trend_polls_df,\n",
    "                inval_recommended_polls_df,\n",
    "                inval_trend_polls_df,\n",
    "            ],\n",
    "            ignore_index=False,\n",
    "        )\n",
    "\n",
    "    recommended_polls_df = recommended_polls_df.reset_index(drop=True)\n",
    "    recommended_polls_df = recommended_polls_df[\"id\"].tolist()\n",
    "    recommended_polls_df = remove_duplicates(recommended_polls_df)\n",
    "\n",
    "    return recommended_polls_df\n",
    "\n",
    "\n",
    "def validate_polls_v3(polls_df, df_name, verbose=True):\n",
    "    # Convert the 'timestamp_column' to a pandas datetime object\n",
    "    # polls_df[\"liive\"] = pd.to_datetime(\n",
    "    #    polls_df[\"endedAt\"], format=\"%Y-%m-%dT%H:%M:%S.%fZ\", utc=True\n",
    "    # )\n",
    "\n",
    "    # polls_df[\"endedAt\"] = pd.to_datetime(polls_df[\"endedAt\"], utc=True)\n",
    "\n",
    "    # Create a boolean mask based on whether the timestamp has passed\n",
    "    # mask = (polls_df[\"endedAt\"] < current_timestamp) & polls_df[\"valid\"]\n",
    "\n",
    "    # Condition 2: Check if the time in \"endedAt\" column has not passed\n",
    "    current_timestamp = pd.Timestamp.utcnow()\n",
    "\n",
    "    mask = (\n",
    "        # (polls_df[\"endedAt\"].notna())\n",
    "        # (polls_df[\"liive\"] < current_timestamp)\n",
    "        (\n",
    "            pd.to_datetime(\n",
    "                polls_df[\"endedAt\"],\n",
    "                format=\"ISO8601\",\n",
    "                utc=True,\n",
    "            )\n",
    "            >= current_timestamp\n",
    "        )\n",
    "        & polls_df[\"valid\"]\n",
    "    )\n",
    "\n",
    "    # Apply the mask to the DataFrame\n",
    "    valid_polls = polls_df[mask]\n",
    "    invalid_polls = polls_df[~mask]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"valid_{df_name}_polls: {len(valid_polls)}\")\n",
    "        print(f\"expired_{df_name}_polls: {len(invalid_polls)}\")\n",
    "\n",
    "    return valid_polls, invalid_polls\n",
    "\n",
    "\n",
    "def order_v4(\n",
    "    recommended_polls_df=None,\n",
    "    trend_polls_df=None,\n",
    "    live_polls_flag=0,\n",
    "    verbose=True,\n",
    "):\n",
    "    val_recommended_polls_df, inval_recommended_polls_df = validate_polls_v3(\n",
    "        recommended_polls_df, \"recommended\", verbose\n",
    "    )\n",
    "\n",
    "    val_trend_polls_df, inval_trend_polls_df = validate_polls_v3(\n",
    "        trend_polls_df, \"trend\", verbose\n",
    "    )\n",
    "\n",
    "    if live_polls_flag:\n",
    "        recommended_polls_df = pd.concat(\n",
    "            [\n",
    "                val_recommended_polls_df,\n",
    "                val_trend_polls_df,\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    else:\n",
    "        recommended_polls_df = pd.concat(\n",
    "            [\n",
    "                val_recommended_polls_df,\n",
    "                val_trend_polls_df,\n",
    "                inval_recommended_polls_df,\n",
    "                inval_trend_polls_df,\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    recommended_polls_df = recommended_polls_df.reset_index(drop=True)\n",
    "    recommended_polls_df = recommended_polls_df[\"id\"].tolist()\n",
    "    recommended_polls_df = remove_duplicates(recommended_polls_df)\n",
    "\n",
    "    return recommended_polls_df\n",
    "\n",
    "\n",
    "def list_to_df(polls_list, polls_df):\n",
    "    # Filter the DataFrame based on the id_list\n",
    "    filtered_df = polls_df[polls_df[\"id\"].isin(polls_list)]\n",
    "\n",
    "    # Create a dictionary to preserve the order\n",
    "    order_dict = {id: idx for idx, id in enumerate(polls_list)}\n",
    "\n",
    "    # Sort the filtered DataFrame based on the order\n",
    "    filtered_df[\"order\"] = filtered_df[\"id\"].map(order_dict)\n",
    "    filtered_df = filtered_df.sort_values(\"order\")\n",
    "\n",
    "    # Drop the 'order' column if not needed\n",
    "    filtered_df = filtered_df.drop(columns=[\"order\"])\n",
    "\n",
    "    # Reset the index if needed\n",
    "    filtered_df = filtered_df.reset_index(drop=True)\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def filter_polls(row, user_limitations):\n",
    "    if (\n",
    "        row[\"pollType\"] == \"Public\"\n",
    "        and isinstance(row.get(\"pollLimitations\"), dict)\n",
    "        and all(k in user_limitations for k in [\"Location\", \"Gender\", \"Age\"])\n",
    "        # and is_within_x_days_liifetime(row[\"createdAt\"])\n",
    "    ):\n",
    "        user_location = user_limitations.get(\"Location\")\n",
    "\n",
    "        allowed_locations = row.get(\"pollLimitations\").get(\"allowedLocations\")\n",
    "        if len(allowed_locations) == 0 or any(\n",
    "            user_location == loc for loc in allowed_locations\n",
    "        ):\n",
    "            allowed_gender = row[\"pollLimitations\"][\"allowedGender\"]\n",
    "            user_gender = user_limitations[\"Gender\"]\n",
    "            if allowed_gender == \"All\" or allowed_gender == user_gender:\n",
    "                allowed_age_range = row[\"pollLimitations\"][\"allowedAgeRange\"]\n",
    "                user_age = user_limitations[\"Age\"]\n",
    "                if (\n",
    "                    allowed_age_range[\"minimumAge\"]\n",
    "                    <= user_age\n",
    "                    <= allowed_age_range[\"maximumAge\"]\n",
    "                ):\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_allowed_private_polls(\n",
    "    params,\n",
    "    # url=\"https://dev.pollett.io/api/Recommend/Polls/GetPrivatePollThatUserCanSee\",\n",
    "    url,\n",
    "):\n",
    "    # API URL\n",
    "    # url = \"https://dev.pollett.io/api/Recommend/Polls/GetPrivatePollThatUserCanSee\"\n",
    "\n",
    "    # Parameters\n",
    "    # params = {\"userId\": \"bbe64b34-ba34-4fbd-a62f-e6c84c0423b4\"}\n",
    "\n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        # allowed_polls_list = response.json().get(\"data\")\n",
    "        return response.json().get(\"data\")\n",
    "        # Process the data as needed\n",
    "\n",
    "    else:\n",
    "        # Handle the error\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#    pd.set_option(\"display.max_colwidth\", None)\n",
    "#    pd.set_option(\"display.max_columns\", None)\n",
    "#\n",
    "#    path = Path(__file__).parent.parent.parent.resolve()\n",
    "#    path = str(path) + \"/data/elas_polls.json\"\n",
    "#    polls_list = []\n",
    "#    # polls = pd.read_json(str(path) + \"/data/elas_polls.json\")\n",
    "#    with open(path, \"r\") as infile:\n",
    "#        polls = json.load(infile)\n",
    "#        for poll in polls:\n",
    "#            # poll = poll[\"_source\"]\n",
    "#            # print(f\"poll:\\n{poll}\")\n",
    "#            polls_list.append(poll)\n",
    "#\n",
    "#    polls = pd.DataFrame.from_records(polls_list)\n",
    "#\n",
    "#    polls = encode_topics(polls)\n",
    "#    print(polls)\n",
    "#    # check_column_type(polls, 4, str)\n",
    "#    tf_idf_matrix = create_tf_idf_matrix(polls, \"question\")\n",
    "#    cosine_similarity_matrix = calc_cosine_similarity_matrix(\n",
    "#        tf_idf_matrix, tf_idf_matrix\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this attribute for each user to a database\n",
    "# useful_recommendation_ratio = interacted_rp / num_of_represented_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test current algorithm's functionality and quality\n",
    "# mean_rate = all_useful_recommendation_rate / num_of_rates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
